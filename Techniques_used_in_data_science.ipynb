{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Techniques used in data science.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNc77rzHEyO4psevm83bqHw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chudy246/Data-Science/blob/master/Techniques_used_in_data_science.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFzmm_WM40_O"
      },
      "source": [
        "# Techniques used in data science\r\n",
        "Overview of common techniques when dealing with an analysis problem.\r\n",
        "Some parts have only short bulletpoints, some extensive summaries.\r\n",
        "Knowledge provided here is based on many articles from Analytics Vidhya, Kaggle courses and other sources.\r\n",
        "\r\n",
        "Overall expected use of this piece is to be a compact summary of all methods, that one can use for analysis. Enjoy!\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjZma78VYAec"
      },
      "source": [
        "# Explorative analysis\r\n",
        "- Visualisation (Heatmaps, Kernel density estimation)\r\n",
        "  - from seaborn: **lineplot**, **boxplot**, **barplot**, **heatmap**, **scatterplot**, **regplot**(scatter plot with Linear Regression(LR)), **lmplot** (scatterplot with colors with LR), **swarmplot**(categorical scatter plot), **distplot**(histogram), **kdeplot**(!)(Kernel density estimation (KDE) plot), **jointplot**(!!!)(2d-kde plot)\r\n",
        "\r\n",
        "Greatplot of potential uses of visualization:\r\n",
        "![uses of visualization](https://i.imgur.com/2VmgDnF.png)\r\n",
        "\r\n",
        "- statistical techniques(t/z- tests, ANOVA, $\\chi^2$, $R^2$)\r\n",
        "\r\n",
        "There are 7 important steps to explorative analysis:\r\n",
        "- Variable Identification\r\n",
        "- Univariate Analysis\r\n",
        "- Bi-variate Analysis\r\n",
        "- Missing values treatment\r\n",
        "- Outlier treatment\r\n",
        "- Variable transformation\r\n",
        "- Variable creation\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7g1_wwNbHE1"
      },
      "source": [
        "## **Variable identification**\r\n",
        "First, identify **Predictor** (Input) and **Target** (output) variables. Next, identify the **data type** and **category** of the variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC3O9vq7bA9F"
      },
      "source": [
        "##  **Univariate Analysis**\r\n",
        "Explore variables one by one. Techniques are different depending on types of variables.\r\n",
        "1. **Continuous variables**\r\n",
        "\r\n",
        "  Understand the central tendency and spread of the variable. Central tendency can be checked by use of **mean, median, mode, min, max**. The spread by **range, quartiles, IQR, variance, standard deviation, skewness and kurtosis**. Variable can be visualized using **histogram** and **box plot**.\r\n",
        "2. **Categorical variables**\r\n",
        "\r\n",
        "  Use **frequency tables** (counts and percentages) and **bar chart** for visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La_Kc9Tia904"
      },
      "source": [
        "## **Bi-variate analysis**\r\n",
        "### **Continuous-continuous**\r\n",
        "For continuous-continuous pair use **scatter plots**. **Correlation** can be used to quantify relation between variables.\r\n",
        "### **Categorical-categorical**\r\n",
        "  - **Two-way table**: We can start analyzing the relationship by creating a two-way table of count and count%. The rows represents the category of one variable and the columns represent the categories of the other variable. We show count or count% of observations available in each combination of row and column categories.\r\n",
        "  - **Stacked column chart**: This method is more of a visual form of Two-way table.\r\n",
        "  - **Chi-Square test**: This test is used to derive the statistical significance of relationship between the variables. Also, it tests whether the evidence in the sample is strong enough to generalize that the relationship for a larger population as well. Chi-square is based on the difference between the expected and observed frequencies in one or more categories in the two-way table. It returns probability for the computed chi-square distribution with the degree of freedom. Other statistical Measures used to analyze the power of relationship are **Cramer’s V** for Nominal Categorical Variable and **Mantel-Haenszed Chi-Square** for ordinal categorical variable.\r\n",
        "\r\n",
        "### **Continuous-Categorical**\r\n",
        "While exploring relation between categorical and continuous variables, we can draw **box plots** for each level of categorical variables. If levels are small in number, it will not show the statistical significance. To look at the **statistical significance** we can perform **Z-test, T-test** (Either test assess whether mean of two groups are statistically different from each other or not. T-test is used when number of observations is small for both categories, e.g less than 30.) or **ANOVA** (It assesses whether the average of more than two groups is statistically different.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34Na4gm7azde"
      },
      "source": [
        "## **Missing value treatment**\r\n",
        "The missing values might happen at two important stages:\r\n",
        "### Data Extraction: \r\n",
        "It is possible that there are problems with extraction process. In such cases, we should double-check for correct data with data guardians. Some hashing procedures can also be used to make sure data extraction is correct. Errors at data extraction stage are typically easy to find and can be corrected easily as well.\r\n",
        "### Data Collection: \r\n",
        "These errors occur at time of data collection and are harder to correct. They can be categorized in four types:\r\n",
        "- **Missing completely at random**: This is a case when the probability of missing variable is same for all observations.\r\n",
        "- **Missing at random**: This is a case when variable is missing at random and missing ratio varies for different values / level of other input variables.\r\n",
        "- **Missing that depends on unobserved predictors**: This is a case when the missing values are not random and are related to the unobserved input variable. For example: In a medical study, if a particular diagnostic causes discomfort, then there is higher chance of drop out from the study. This missing value is not at random unless we have included “discomfort” as an input variable for all patients.\r\n",
        "- **Missing that depends on the missing value itself**: This is a case when the probability of missing value is directly correlated with missing value itself. For example: People with higher or lower income are likely to provide non-response to their earning.\r\n",
        "\r\n",
        "### **Methods to treat missing values**\r\n",
        "1. **Deletion**:  It is of two types: List Wise Deletion and Pair Wise Deletion.\r\n",
        "\r\n",
        "  - In **list wise deletion**, we delete observations where any of the variable is missing. Simplicity is one of the major advantage of this method, but this method reduces the power of model because it reduces the sample size.\r\n",
        "\r\n",
        "  - In **pair wise deletion**, we perform analysis with all cases in which the variables of interest are present. Advantage of this method is, it keeps as many cases available for analysis. One of the disadvantage of this method, it uses different sample size for different variables.\r\n",
        "\r\n",
        "  - Deletion methods are used when the nature of missing data is **“Missing completely at random”** else non random missing values can bias the model output.\r\n",
        "\r\n",
        "2. **Mean/ Mode/ Median Imputation**: It consists of replacing the missing data for a given attribute by the mean or median (quantitative attribute) or mode (qualitative attribute) of all known values of that variable. It can be of two types:\r\n",
        "\r\n",
        "  - **Generalized Imputation**: In this case, we calculate the mean or median for all non missing values of that variable then replace missing value with mean or median.\r\n",
        "\r\n",
        "  - **Similar case Imputation**: In this case, we calculate average for gender “Male” (29.75) and “Female” (25) individually of non missing values then replace the missing value based on gender. (In place of gender choose any category)\r\n",
        "\r\n",
        "3. **Prediction Model**: Here, we create a predictive model to estimate values that will substitute the missing data.  In this case, we divide our data set into two sets: One set with no missing values for the variable and another one with missing values. First data set become training data set of the model while second data set with missing values is test data set and variable with missing values is treated as target variable. Next, we create a model to predict target variable based on other attributes of the training data set and populate missing values of test data set.We can use regression, ANOVA, Logistic regression and various modeling technique to perform this. There are 2 drawbacks for this approach:\r\n",
        "  - The model estimated values are usually more well-behaved than the true values\r\n",
        "  - If there are no relationships with attributes in the data set and the attribute with missing values, then the model will not be precise for estimating missing values.\r\n",
        "\r\n",
        "4. **KNN imputation**: In this method of imputation, the missing values of an attribute are imputed using the given number of attributes that are most similar to the attribute whose values are missing. The similarity of two attributes is determined using a distance function. It is also known to have certain advantage & disadvantages. <br> **Advantages**:\r\n",
        "  - k-nearest neighbour can predict both qualitative & quantitative attributes\r\n",
        "  - Creation of predictive model for each attribute with missing data is not required\r\n",
        "  - Attributes with multiple missing values can be easily treated\r\n",
        "  - Correlation structure of the data is taken into consideration<br>\r\n",
        "\r\n",
        "  **Disadvantages**:\r\n",
        "  - KNN algorithm is very time-consuming in analyzing large database. It searches through all the dataset looking for the most similar instances.\r\n",
        "  - Choice of k-value is very critical. Higher value of k would include attributes which are significantly different from what we need whereas lower value of k implies missing out of significant attributes.\r\n",
        "\r\n",
        "5. **Treat separately y adding a categorical variable**: We can impute the values and then add a column which will tell if the values were imputed or not. Model then can take this into account during prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IImF-XFlallN"
      },
      "source": [
        "## **Techniques of Outlier Detection and Treatment**\r\n",
        "Outlier is an observation that appears far away and diverges from an overall pattern in a sample. Outlier can be of two types:\r\n",
        "- **Univariate**: these can be found when we look at distribution of a single variable\r\n",
        "- **Multivariate**: these outliers are outliers in an n-dimensional space. In order to find them, you have to look at distributions in multi-dimensions.\r\n",
        "\r\n",
        "### Causes of outliers\r\n",
        "Causes can classified into several categories\r\n",
        "1. **Artificial (Error)/ Non-natural** of which we can distinguish:\r\n",
        "  - Data Entry Errors: Human errors such as errors caused during data collection, recording, or entry can cause outliers in data.\r\n",
        "  - Measurement Error: It is the most common source of outliers. This is caused when the measurement instrument used turns out to be faulty.\r\n",
        "  - Experimental Error: Another cause of outliers is experimental error. For example: In a 100m sprint of 7 runners, one runner missed out on concentrating on the ‘Go’ call which caused him to start late. Hence, this caused the runner’s run time to be more than other runners. His total run time can be an outlier.\r\n",
        "  - Intentional Outlier: This is commonly found in self-reported measures that involves sensitive data.\r\n",
        "  - Data Processing Error: Whenever we perform data mining, we extract data from multiple sources. It is possible that some manipulation or extraction errors may lead to outliers in the dataset.\r\n",
        "  - Sampling error: For instance, we have to measure the height of athletes. By mistake, we include a few basketball players in the sample. This inclusion is likely to cause outliers in the dataset.\r\n",
        "2. **Natural**. Of course, if outlier is not artificial, it is natural.\r\n",
        "\r\n",
        "### Impact of outliers\r\n",
        "- It increases the error variance and reduces the power of statistical tests\r\n",
        "- If the outliers are non-randomly distributed, they can decrease normality\r\n",
        "- They can bias or influence estimates that may be of substantive interest\r\n",
        "- They can also impact the basic assumption of Regression, ANOVA and other statistical model assumptions.\r\n",
        "\r\n",
        "### Detecting outliers\r\n",
        "Outliers are most commonly and easily seen via visualisation, e.g. using a **Box-Plot**, **Histogram** or a **Scatter plot**. There are also common rules of thumb:\r\n",
        "- Any value, which is beyond the range of -1.5 x IQR to 1.5 x IQR\r\n",
        "- Use capping methods. Any value which out of range of 5th and 95th percentile can be considered as outlier\r\n",
        "- Data points, three or more standard deviation away from mean are considered outlier\r\n",
        "- Outlier detection is merely a special case of the examination of data for influential data points and it also depends on the business understanding\r\n",
        "- Bivariate and multivariate outliers are typically measured using either an index of influence or leverage, or distance. Popular indices such as Mahalanobis’ distance and Cook’s D are frequently used to detect outliers.\r\n",
        "- In SAS, we can use PROC Univariate, PROC SGPLOT. To identify outliers and influential observation, we also look at statistical measure like STUDENT, COOKD, RSTUDENT and others.\r\n",
        "\r\n",
        "### Removing outliers\r\n",
        "- **Deleting observations**: We delete outlier values if it is due to data entry error, data processing error or outlier observations are very small in numbers. We can also use trimming at both ends to remove outliers.\r\n",
        "- **Transforming and binning values**: Transforming variables can also eliminate outliers. Natural log of a value reduces the variation caused by extreme values. Binning is also a form of variable transformation. Decision Tree algorithm allows to deal with outliers well due to binning of variable. We can also use the process of assigning weights to different observations.\r\n",
        "- **Imputing**: Like imputation of missing values, we can also impute outliers. We can use mean, median, mode imputation methods. Before imputing values, we should analyse if it is natural outlier or artificial. If it is artificial, we can go with imputing values. We can also use statistical model to predict values of outlier observation and after that we can impute it with predicted values.\r\n",
        "- **Treat separately**: If there are significant number of outliers, we should treat them separately in the statistical model. One of the approach is to treat both groups as two different groups and build individual model for both groups and then combine the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpOfvjNOaap9"
      },
      "source": [
        "## **Feature engineering**\r\n",
        "Can be divided into two main steps:\r\n",
        "- Variable transformation\r\n",
        "- Variable/ Feature creation\r\n",
        "\r\n",
        "### **Variable transformation**\r\n",
        "It's a process of replacing a variable by a function of it.\r\n",
        "### When should we use variable transformation:\r\n",
        "- To **change the scale** of a variable or standardize the values of a variable for better understanding. While this transformation is a must if you have data in different scales, this transformation does not change the shape of the variable distribution\r\n",
        "- To **transform complex non-linear relationships into linear relationships**. Existence of a linear relationship between variables is easier to comprehend compared to a non-linear or curved relation and also improves prediction. Scatter plot can be used to find the relationship between two continuous variables. Log transformation is one of the commonly used transformation technique used in these situations.\r\n",
        "- **Symmetric distribution is preferred over skewed distribution** as it is easier to interpret and generate inferences. Some modeling techniques requires normal distribution of variables. So, whenever we have a skewed distribution, we can use transformations which reduce skewness. For right skewed distribution, we take square / cube root or logarithm of variable and for left skewed, we take square / cube or exponential of variables\r\n",
        "- Variable Transformation is also done from an **implementation point of view** (Human involvement). Let’s understand it more clearly. In one of my project on employee performance, I found that age has direct correlation with performance of the employee i.e. higher the age, better the performance. From an implementation stand point, launching age based programme might present implementation challenge. However, categorizing the sales agents in three age group buckets of under 30 years, 30-45 years and above 45 and then formulating three different strategies for each group is a judicious approach. This categorization technique is known as Binning of Variables.\r\n",
        "\r\n",
        "### Common methods of variable transformation\r\n",
        "- **Logarithm**/**Exponential**: Log of a variable is a common transformation method used to change the shape of distribution of the variable on a distribution plot. It is generally used for reducing right skewness of variables. Though, It can’t be applied to zero or negative values as well.\r\n",
        "- **Square / Cube root**/ **2nd/3rd power**: The square and cube root of a variable has a sound effect on variable distribution. However, it is not as significant as logarithmic transformation. Cube root has its own advantage. It can be applied to negative values including zero. Square root can be applied to positive values including zero.\r\n",
        "- **Binning**: It is used to categorize variables. It is performed on original values, percentile or frequency. Decision of categorization technique is based on business understanding. For example, we can categorize income in three categories, namely: High, Average and Low. We can also perform co-variate binning which depends on the value of more than one variables.\r\n",
        "- **feature hashing** (hashing trick): Fast and space-efficient way of vectorizing features\r\n",
        "\r\n",
        "### **Feature / Variable Creation**\r\n",
        "Feature / Variable creation is a process to generate a new variables / features based on existing variable(s). For example, say, we have date(dd-mm-yy) as an input variable in a data set. We can generate new variables like day, month, year, week, weekday that may have better relationship with target variable. This step is used to highlight the hidden relationship in a variable. <br>\r\n",
        "Commonly used techniques:\r\n",
        "- **Creating derived variables**: This refers to creating new variables from existing variable(s) using set of functions or different methods.\r\n",
        "- **Creating dummy variables**: One of the most common application of dummy variable is to convert categorical variable into numerical variables.\r\n",
        "\r\n",
        "\r\n",
        "Other possible ideas for variable transformation/creation:\r\n",
        "- **Create variables for difference in date, time and addresses**: e.g., an applicant who takes days to fill in an application form is likely to be less interested / motivated in the product compared to some one who fills in the same application with in 30 minutes. Similarly, for a bank, time elapsed between dispatch of login details for Online portal and customer logging in might show customers’ willingness to use Online portal.\r\n",
        "Another example is that a customer living closer to a bank branch is more likely to have a higher engagement than a customer living far off.\r\n",
        "- **Create new ratios and proportions**: e.g., Input / Output (past performance), productivity, efficiency and percentages\r\n",
        "- **Apply standard transformations**: e.g., log, exponential, quadratic and trigonometric transforms. In scientific data it might be also Fourier transforms, convolutions.\r\n",
        "- **Include effect of influencer** (e.g. survivorship bias): Influencer can impact on the behaviour of your study significantly. Influencer could be of various form and sizes. It could be an employee of your Organization, agent of your Organization or a customer of your Organization. Bringing the impact of these related entities can improve the models significantly. For example, a loan initiated by a sub-set of brokers (and not all brokers) might be more likely to be transferred to a different entity after the lock-in period. Similarly, there might be a sub set of Sales personnel involved who do a higher cross-sell to their customers.\r\n",
        "- **Check variables for seasonality and create the model for right period**:A lot of businesses face some kind of seasonality. It could be driven by tax benefits, festive season or weather. If this is the case, you need to make sure that the data and variables are chosen for the right period."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEh9tcvTY2W8"
      },
      "source": [
        "# Preprocess (Some included in explorative analysis)\r\n",
        "- cleaning (filling missing values, encoding category variables)\r\n",
        "- variable transformations\r\n",
        "- Dimensionality reduction\r\n",
        "- Feature engineering\r\n",
        "- Principal Component Analysis\r\n",
        "- Factor Analysis\r\n",
        "- Data Augmentation\r\n",
        "- Oversampling/Undersampling\r\n",
        "- Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVQF6QsHZM9f"
      },
      "source": [
        "# Supervised learning\r\n",
        "- naive bayes\r\n",
        "- Linear models\r\n",
        "- linear and logistic regressions (feature selection using p-values and other statistics)\r\n",
        "- Decision trees\r\n",
        "- Random forest\r\n",
        "- Neural Networks (MLP, CNN, RNN and more)\r\n",
        "- Ensemble techniques (Stacking, Blending. Random Forest, XGBoost):\r\n",
        "  1. **Random Forest**: We choose randomly a subset of training data and features to create a tree. We repeat that process to create a forest. In the end the prediction is created by taking the mean/mode of predictions of individual trees.\r\n",
        "  2. **XGBoost**: We choose a first tree to classify data. Then using gradient calculations we optimize its parameters.\r\n",
        "\r\n",
        "- Bagging and Boosting algorithms\r\n",
        "- Hyperparameter tuning\r\n",
        "- cross validation\r\n",
        "- Support Vector Machines\r\n",
        "- Hyperparameter optimization (e.g. grid search)\r\n",
        "- Data leakage(target leakage & train-test contamination)\r\n",
        "  1. **Target leakage** occurs when your predictors include data that will not be available at the time you make predictions. It is important to think about target leakage in terms of the timing or chronological order that data becomes available, not merely whether a feature helps make good predictions. To prevent this type of data leakage, any variable updated (or created) after the target value is realized should be excluded.\r\n",
        "  2. **Train-test contamination**: e.g. using some preprocessing steps that use information from validation data (imputing, complex feature engineering). If your validation is based on a simple train-test split, exclude the validation data from any type of fitting, including the fitting of preprocessing steps. This is easier if you use scikit-learn pipelines. When using cross-validation, it's even more critical that you do your preprocessing inside the pipeline!\r\n",
        "  \r\n",
        "  Data leakage can be multi-million dollar mistake in many data science applications. Careful separation of training and validation data can prevent train-test contamination, and pipelines can help implement this separation. Likewise, a combination of caution, common sense, and data exploration can help identify target leakage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvVjRAegaMHO"
      },
      "source": [
        "# Unsupervised learning\r\n",
        "- clustering algorithms (e.g. k-nearest neighbours, hierarchical)\r\n",
        "\r\n",
        "## For Natural Language Processing\r\n",
        "- regular expressions\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5EDV10taPuU"
      },
      "source": [
        "# Interesting use examples\r\n",
        "1. Getting Rich With Cryptocurrencies?\r\n",
        "  \r\n",
        "  Your friend, who is also a data scientist, says he has built a model that will let you turn your bonus into millions of dollars. Specifically, his model predicts the price of a new cryptocurrency (like Bitcoin, but a newer one) one day ahead of the moment of prediction. His plan is to purchase the cryptocurrency whenever the model says the price of the currency (in dollars) is about to go up. The most important features in his model are:\r\n",
        "  - Current price of the currency\r\n",
        "  - Amount of the currency sold in the last 24 hours\r\n",
        "  - Change in the currency price in the last 24 hours\r\n",
        "  - Change in the currency price in the last 1 hour\r\n",
        "  - Number of new tweets in the last 24 hours that mention the currency\r\n",
        "\r\n",
        "  The value of the cryptocurrency in dollars has fluctuated up and down by over 100 in the last year, and yet this model's average error is less than 1. He says this is proof his model is accurate, and you should invest with him, buying the currency whenever the model says it is about to go up. Is he right? If there is a problem with his model, what is it?\r\n",
        "\r\n",
        "  **Answer**: There is no source of leakage here. These features should be available at the moment you want to make a predition, and they're unlikely to be changed in the training data after the prediction target is determined. But, the way he describes accuracy could be misleading if you aren't careful. If the price moves gradually, today's price will be an accurate predictor of tomorrow's price, but it may not tell you whether it's a good time to invest. For instance, if it is 100 today, a model predicting a price of 100 tomorrow may seem accurate, even if it can't tell you whether the price is going up or down from the current price. A better prediction target would be the change in price over the next day. If you can consistently predict whether the price is about to go up or down (and by how much), you may have a winning investment opportunity.\r\n",
        "\r\n",
        "  **Conclusion**: Need to be careful when predicting the price of slow-moving metric in short amount of time. A metric such as \"does price go up or down\" instead of absolute values difference might be more suitable to check accuracy of this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhsvnSWMaWlW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}